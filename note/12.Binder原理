我们需要考虑下面几点，这几点不仅是Binder的设计需要考虑的，也是我们在程序架构上需要考虑的几点。
1.性能：
        在Linux系统中，进程间传输数据性能最好的方式就是共享内存，或是以共享内存为原理衍生出来的技术，如mmap内存映射。
    共享内存或者mmap都只需要进行一次数据拷贝，即把想要传输的数据拷贝到共享或者映射的内存区域中，另一个共享或者映
    射了这段内存的进程就可以直接使用内存中的数据了，其他的IPC传输都需要两次拷贝，即将传输的数据拷贝的指定的内存
    区域（一般是内核空间），然后又将指定的内存区域的数据拷贝到需要通信的进程的内存中去。所以为了性能考虑，Binder
    在设计时，采用了mmap内存映射这种方式来进行数据的传输

2.安全：
        Binder设计中为了安全性的考虑， 天然支持携带进程ID，这样在进程间通信时，可以通过进程ID进行相应的权限控制。
    并且Binder是CS架构，Servcer更容易对Client的访问权限进行控制。

3.可扩展和低耦合：
        在CS架构上，Clinet和Server都容易扩展，想要扩展通信，只需要增加Client或者Server就可以了，而不用去管中间的通信流程。


//////////////////////////////////////////////////Linux基础知识//////////////////////////////////////////////////////

1.用户空间与内核空间
        Linux系统在32位机上为每个Linux进程分配了4G（2^32）的虚拟内存,其中有3GB的虚拟地址供该进程使用，称为用户空间，
    还有1GB留给其页表和其他内核数据，称为内核空间，内核空间是所有进程共享的。

2.mmap内存映射
        mmap可以将一个文件或者其它对象映射进进程的用户空间，这种情况下，可以像使用自己进程的内存一样使用这段内存。
     Linux系统的mmap函数原型是这样的.
        非mmap或者内存共享的Linux IPC机制常用的通信方式如下，数据发送进程的用户空间数据通过copy_from_user，复制到内核空间，
     由于内核空间是所有进程共享，所以内核通过调用copy_to_user将数据写入到数据接收进程，通过两次拷贝的方式，完成了IPC的通信。
        通过mmap或者内存共享的Linux IPC机制，直接将同一段内存映射到数据发送进程和数据接收进程的用户空间，这样数据发送进程
     只需要将数据拷贝到共享的内存区域，数据接收进程就可以直接使用数据了。

/////////////////////////////////////////////Binder的架构设计////////////////////////////////////////////////////////
Binder主要由这几部分组成
1.Binder设备驱动
        Binder驱动设备是真正分配内存空间用来存放通信数据的部分，在Binder的架构中，Clinet端发送的数据拷贝到Binder驱动设备分配的内存空间中，
    Server会通过mmap将Binder驱动设备中分配的内存映射到自己进程的用户空间中，映射完成后，Server在用户空间就可以直接读取Binder驱动中存放数据的这段内存了。

2.Client端，数据发送端
        Client端是数据发送方，它会通过I/O函数，ioctl陷入内核，通知binder驱动将client端的数据通过copy_from_user函数拷贝过来，并存放在binder驱动的内存中。

3.Server端，数据接收端
        Server是数据接收方，它接收数据方式的方式是映射Binder驱动中存放Clinet端数据的内存到自己的用户空间，这样就可以直接使用这段内存了。

4.ServiceManager
        ServiceManager是专门用来管理Server端的，Client端想要和Server通信，必须知道Server的映射的内存地址，这样才能往这这段内存中拷贝数据，
    但是我们不可能知道所有Server端的地址，所以这个时候，我们只需要知道ServiceManager的地址，在ServiceManager中寻找其他Server的地址就
    可以了。所以ServiceManager有点类似DNS服务器。